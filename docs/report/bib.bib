@INPROCEEDINGS{melody-extraction,
author={G. {Ozcan} and C. {Isikhan} and A. {Alpkocak}},
booktitle={Seventh IEEE International Symposium on Multimedia (ISM'05)},
title={Melody extraction on MIDI music files},
year={2005},
volume={},
number={},
pages={8 pp.-},
keywords={music;content-based retrieval;acoustic signal processing;feature extraction;monophonic melody extraction;MIDI music file;MIDI channel;musical instrument digital interface;melodic information;pitch histogram;skyline algorithm;variable music style;music information retrieval;Multiple signal classification;Music information retrieval;Data mining;Histograms;Clustering algorithms;Testing;Content based retrieval;Databases;Internet;Entropy;Melody Extraction;pitch histogram;Music Information Retrieval},
doi={10.1109/ISM.2005.77},
ISSN={},
month={Dec},}

@Article{genre-classification,
author="Cataltepe, Zehra
and Yaslan, Yusuf
and Sonmez, Abdullah",
title="Music Genre Classification Using MIDI and Audio Features",
journal="EURASIP Journal on Advances in Signal Processing",
year="2007",
month="Dec",
day="01",
volume="2007",
number="1",
pages="036409",
abstract="We report our findings on using MIDI files and audio features from MIDI, separately and combined together, for MIDI music genre classification. We use McKay and Fujinaga's 3-root and 9-leaf genre data set. In order to compute distances between MIDI pieces, we use normalized compression distance (NCD). NCD uses the compressed length of a string as an approximation to its Kolmogorov complexity and has previously been used for music genre and composer clustering. We convert the MIDI pieces to audio and then use the audio features to train different classifiers. MIDI and audio from MIDI classifiers alone achieve much smaller accuracies than those reported by McKay and Fujinaga who used not NCD but a number of domain-based MIDI features for their classification. Combining MIDI and audio from MIDI classifiers improves accuracy and gets closer to, but still worse, accuracies than McKay and Fujinaga's. The best root genre accuracies achieved using MIDI, audio, and combination of them are 0.75, 0.86, and 0.93, respectively, compared to 0.98 of McKay and Fujinaga. Successful classifier combination requires diversity of the base classifiers. We achieve diversity through using certain number of seconds of the MIDI file, different sample rates and sizes for the audio file, and different classification algorithms.",
issn="1687-6180",
doi="10.1155/2007/36409",
url="https://doi.org/10.1155/2007/36409"
}

@article{student-perspectives,
author = { Samuel   Airy  and  Judy M.   Parr },
title = {MIDI, Music and Me: Students' Perspectives on Composing with MIDI},
journal = {Music Education Research},
volume = {3},
number = {1},
pages = {41-49},
year  = {2001},
publisher = {Routledge},
doi = {10.1080/14613800020029941},

URL = {
        https://doi.org/10.1080/14613800020029941

},
eprint = {
        https://doi.org/10.1080/14613800020029941

}

}

@INPROCEEDINGS{visualization,
author={S. M. {Smith} and G. N. {Williams}},
booktitle={Proceedings. Visualization '97 (Cat. No. 97CB36155)},
title={A visualization of music},
year={1997},
volume={},
number={},
pages={499-503},
keywords={music;data visualisation;colour graphics;music visualization;music notation;color;3D space;music data representation;Multiple signal classification;Music;Instruments;Data visualization;Timbre;Computer science;Rhythm;Shape},
doi={10.1109/VISUAL.1997.663931},
ISSN={},
month={Oct},
}

@InProceedings{emotion,
author="Chen, Chin-Han
and Weng, Ming-Fang
and Jeng, Shyh-Kang
and Chuang, Yung-Yu",
editor="Satoh, Shin'ichi
and Nack, Frank
and Etoh, Minoru",
title="Emotion-Based Music Visualization Using Photos",
booktitle="Advances in Multimedia Modeling",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="358--368",
abstract="Music players for personal computers are often featured with music visualization by generating animated patterns according to the music's low-level features such as loudness and spectrum. This paper proposes an emotion-based music player which synchronizes visualization (photos) with music based on the emotions evoked by auditory stimulus of music and visual content of visualization. For emotion detection from photos, we collected 398 photos with their emotions annotated by 496 users through the web. With these annotations, a Bayesian classification method is proposed for automatic photo emotion detection. For emotion detection from music, we adopt an existing method. Finally, for composition of music and photos, in addition to matching high-level emotions, we also consider low-level feature harmony and temporal visual coherence. It is formulated as an optimization problem and solved by a greedy algorithm. Subjective evaluation shows emotion-based music visualization enriches users' listening experiences.",
isbn="978-3-540-77409-9"
}

@online{muse-score,
  title = {MuseScore},
  year = 2019,
  url = {http://musescore.org},
  urldate = {2019-04-20}
}
@online{garage-band,
  title = {GarageBand},
  year = 2019,
  url = {https://www.apple.com/mac/garageband/},
  urldate = {2019-04-20}
}
